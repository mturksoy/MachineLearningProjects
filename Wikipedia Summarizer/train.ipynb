{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92c49ae4-eab1-414e-830a-c2a2311f8393",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
    "from datasets import Dataset\n",
    "from evaluate import load\n",
    "import re\n",
    "import pandas as pd\n",
    "import warnings \n",
    "warnings.simplefilter(action='ignore', category=Warning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d7ac7e6-4e17-4d78-badb-a0783ab3de38",
   "metadata": {},
   "outputs": [],
   "source": [
    "splits = {\n",
    "    'train': 'data/train-00000-of-00001-87e767a83d108945.parquet',\n",
    "    'validation': 'data/validation-00000-of-00001-2d1ce84ca498cf0b.parquet'\n",
    "}\n",
    "dataset = pd.read_parquet(\"hf://datasets/musabg/wikipedia-tr-summarization/\" + splits[\"train\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f5eb970-3daa-4e3e-ac70-79f22d3fc6ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_sample = dataset.sample(n=4000, random_state=42)\n",
    "\n",
    "reviews_sample = reviews_sample.rename(columns={\"text\": \"article\", \"summary\": \"summary\"})\n",
    "\n",
    "def clean_text(text):\n",
    "    text = re.sub(r\"[^\\w\\s.,!?ğüşöçıİĞÜŞÖÇ]\", \"\", text)  \n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()  \n",
    "    return text\n",
    "\n",
    "reviews_sample[\"article\"] = reviews_sample[\"article\"].apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65f2d3c0-fcf8-45e8-a69c-81f2aad0efec",
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_dataset = Dataset.from_pandas(reviews_sample)\n",
    "\n",
    "train_test_split = hf_dataset.train_test_split(test_size=0.1, seed=42)\n",
    "train_dataset = train_test_split[\"train\"]\n",
    "val_dataset = train_test_split[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f4cb156-31bf-456e-9e61-bad440046009",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"ozcangundes/mt5-small-turkish-summarization\"  \n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "\n",
    "def preprocess_data(examples):\n",
    "    inputs = [\"summarize: \" + doc for doc in examples[\"article\"]]  \n",
    "    outputs = examples[\"summary\"]\n",
    "\n",
    "    model_inputs = tokenizer(inputs, max_length=512, truncation=True, padding=\"max_length\")\n",
    "    labels = tokenizer(outputs, max_length=200, truncation=True, padding=\"max_length\")  \n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "\n",
    "    return model_inputs\n",
    "\n",
    "train_dataset = train_dataset.map(preprocess_data, batched=True, remove_columns=train_dataset.column_names)\n",
    "val_dataset = val_dataset.map(preprocess_data, batched=True, remove_columns=val_dataset.column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cf08ab4-6719-4e8a-ae86-349b4263d78b",
   "metadata": {},
   "outputs": [],
   "source": [
    "rouge = load(\"rouge\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "    result = rouge.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)\n",
    "    \n",
    "    if isinstance(result[\"rouge1\"], dict):\n",
    "        return {\n",
    "            \"rouge1\": result[\"rouge1\"][\"fmeasure\"] * 100,\n",
    "            \"rouge2\": result[\"rouge2\"][\"fmeasure\"] * 100,\n",
    "            \"rougeL\": result[\"rougeL\"][\"fmeasure\"] * 100,\n",
    "        }\n",
    "    return {\n",
    "        \"rouge1\": result[\"rouge1\"] * 100,\n",
    "        \"rouge2\": result[\"rouge2\"] * 100,\n",
    "        \"rougeL\": result[\"rougeL\"] * 100,\n",
    "    }\n",
    "\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"epoch\", \n",
    "    save_strategy=\"epoch\",  \n",
    "    save_total_limit=3,  \n",
    "    learning_rate=2e-5,\n",
    "    lr_scheduler_type=\"linear\",  \n",
    "    warmup_steps=500,\n",
    "    per_device_train_batch_size=6, \n",
    "    per_device_eval_batch_size=6,\n",
    "    num_train_epochs=4,  \n",
    "    weight_decay=0.01,\n",
    "    predict_with_generate=True,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=100,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    fp16=True\n",
    ")\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics \n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b51c6ff5-f32f-4fb9-9ef1-a8ad8ac38c31",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d76f1422-5fab-4aee-8f73-b383950dae50",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model(\"./trained_model\")\n",
    "tokenizer.save_pretrained(\"./trained_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1ddd2a6f-d3da-4354-bbdc-8cdc713011b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n",
      "Your max_length is set to 90, but your input_length is only 74. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=37)\n",
      "Your max_length is set to 90, but your input_length is only 59. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=29)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Makale Özeti: Anderson-darling sınaması, istatistik bilim dalında, bir parametrik olmayan sınaması olup örneklem verilerinin belirli olasılık dağılımı gösterip göstermediğini sınamak için, yani uygunluk iyiliği için kullanılmaktadır. verilmiş n büyüklük sayıda bir örneklem veri serisi, yani , kullanılır ve olasılık dağılımından geldiğinin bu dağılımını tam olarak belirleyen parametre değerinin veya parametreler değerlerinin verilmesi gerekir özellikle dağılımı uygulanması verilerin sonucu verilmiş için incelenmektedir. geçtiğimiz günlerde sınama isteniğinin hesaplanması için kullanılan bir sıra değerleri bulunan değeri bulunur ve kullanılır. anderson-darling istatistiği değeri yani elde edilen , sınama olarak kullanılır ve adı altında belirlenen teorik olasılık dağılımı için p-değeri bulmak kullanılır. olasılık dağılımı simülasyonu için bilgisayarla sayısal hesaplama gerektirir ve gerekeceği gerektirir. belirlenmiş olasılık dağılımına uygunluk sıfır hipotezinin kabul edilmesi sonucudur ancak hesaplanmış değer tablo kritik değerinden büyükse örneklem verileri belirlenmiş gösterir sonucuna varılır\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "import re\n",
    "\n",
    "summarizer = pipeline(\"summarization\", model=\"./trained_model\", tokenizer=\"./trained_model\")\n",
    "\n",
    "def summarize_text_in_chunks(text, chunk_size=3, max_length=90, min_length=30):\n",
    "    \"\"\"\n",
    "    Metni belirli bir sayıda cümleye bölerek özetler.\n",
    "    \n",
    "    Args:\n",
    "        text (str): Özetlenecek metin.\n",
    "        chunk_size (int): Her bir parçada kaç cümle olacağı.\n",
    "        max_length (int): Özetin maksimum uzunluğu (token sayısı).\n",
    "        min_length (int): Özetin minimum uzunluğu (token sayısı).\n",
    "        \n",
    "    Returns:\n",
    "        str: Parçalanmış ve özetlenmiş metinlerin birleşimi.\n",
    "    \"\"\"\n",
    "    # Metni cümlelere böl\n",
    "    sentences = re.split(r'(?<!\\w\\.\\w.)(?<![A-ZİĞÜŞÖÇ][a-zığüşöç]\\.)(?<=\\.|\\?)\\s', text)\n",
    "    \n",
    "    # Cümleleri gruplara ayır\n",
    "    chunks = [' '.join(sentences[i:i+chunk_size]) for i in range(0, len(sentences), chunk_size)]\n",
    "    \n",
    "    # Her bir parçayı özetle\n",
    "    summaries = []\n",
    "    for chunk in chunks:\n",
    "        summary = summarizer(chunk, max_length=max_length, min_length=min_length, do_sample=False)[0][\"summary_text\"]\n",
    "        summaries.append(summary)\n",
    "    \n",
    "    # Tekrar eden kelimeleri temizle\n",
    "    combined_summary = \" \".join(summaries)\n",
    "    combined_summary = clean_redundancies(combined_summary)\n",
    "    \n",
    "    return combined_summary\n",
    "\n",
    "def clean_redundancies(text):\n",
    "    \"\"\"\n",
    "    Metindeki tekrar eden ifadeleri temizler ve anlamsız tekrarları kaldırır.\n",
    "    \n",
    "    Args:\n",
    "        text (str): Temizlenecek metin.\n",
    "        \n",
    "    Returns:\n",
    "        str: Temizlenmiş metin.\n",
    "    \"\"\"\n",
    "    sentences = re.split(r'(?<!\\w\\.\\w.)(?<![A-ZİĞÜŞÖÇ][a-zığüşöç]\\.)(?<=\\.|\\?)\\s', text)\n",
    "    seen_sentences = set()\n",
    "    cleaned_sentences = []\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        cleaned_sentence = remove_repeated_phrases(sentence)\n",
    "        if cleaned_sentence.lower() not in seen_sentences:\n",
    "            seen_sentences.add(cleaned_sentence.lower())\n",
    "            cleaned_sentences.append(cleaned_sentence)\n",
    "    \n",
    "    return \" \".join(cleaned_sentences)\n",
    "\n",
    "def remove_repeated_phrases(sentence):\n",
    "    \"\"\"\n",
    "    Bir cümledeki tekrar eden kelime öbeklerini temizler.\n",
    "    \n",
    "    Args:\n",
    "        sentence (str): Temizlenecek cümle.\n",
    "        \n",
    "    Returns:\n",
    "        str: Tekrarlar temizlenmiş cümle.\n",
    "    \"\"\"\n",
    "    words = sentence.split()\n",
    "    seen_words = set()\n",
    "    cleaned_words = []\n",
    "    \n",
    "    for word in words:\n",
    "        if word not in seen_words:\n",
    "            seen_words.add(word)\n",
    "            cleaned_words.append(word)\n",
    "    \n",
    "    return \" \".join(cleaned_words)\n",
    "\n",
    "def post_process_summary(summary):\n",
    "    \"\"\"\n",
    "    Özet sonrası işleme fonksiyonu.\n",
    "    \n",
    "    Args:\n",
    "        summary (str): İşlenecek özet.\n",
    "        \n",
    "    Returns:\n",
    "        str: İşlenmiş özet.\n",
    "    \"\"\"\n",
    "    summary = summary.strip()\n",
    "    summary = summary.capitalize()\n",
    "    summary = re.sub(r\" +\", \" \", summary)  \n",
    "    summary = re.sub(r\"(?<=[.,])(?=[^\\s])\", \" \", summary)  \n",
    "    return summary\n",
    "\n",
    "article = \"\"\"Anderson-Darling sınaması, istatistik bilim dalında, bir parametrik olmayan istatistik sınaması \n",
    "olup örneklem verilerinin belirli bir olasılık dağılımı gösterip göstermediğini sınamak için, yani uygunluk \n",
    "iyiliği sınaması için, kullanılmaktadır. Bu sınama ilk defa 1952'de Amerikan istatistikçileri T.W.Anderson Jr. \n",
    "ile D.A.Darling tarafından yayınlanmıştır. Hem çok küçük (n le; 25) örneklem sayılı veriler için hem de hacmi \n",
    "200u aşan sanayi kalite kontrol verileri için başarıyla normallik sınaması için kullanıldığı bildirilmiştir. \n",
    "Genel uygunluk iyiliği sınaması Anderson-Darling sınaması bir örneklem verisinin tam olarak belirlenmiş bir olasılık \n",
    "dağılımı gösteren bir anakütleden gelip gelmediğinin sınanması için kullanılır. Verilmiş N büyüklük sayıda bir örneklem \n",
    "veri serisi, yani , kullanılır. Bu serinin sınanmansı için hangi olasılık dağılımından geldiğinin ve bu olasılık dağılımını \n",
    "tam olarak belirleyen parametre değerinin veya parametreler değerlerinin verilmesi gerekir.Anderson-Darling sınaması için sıfır \n",
    "hipotez, her türden uygunluk iyiliği sınaması gibi, örneklem verilerin için tüm parametre değerleri ile iyice belirlenen \n",
    "olasılık dağılımlı anakütleden geldiğidir. Bu sıfır hipotezin çok sınırlı olduğuna dikkat çekilmelidir. Ancak verilmiş \n",
    "parametre veya parametreler için olasılık dağılımı uygulanması incelenmektedir. Eğer sıfır hipotez sınama sonucu ret \n",
    "edilirse, verilerin parametre(ler) ile belirlenmiş dağılıma uymadığı sonucuna varılır. Tekrar edilmelidir ki genel \n",
    "olarak belli bir dağılım ret edilmemektedir; sadece belli parametresi olan dağılım ret edilmektedir.Elde edilen veriler \n",
    "en küçük değerden en büyük değere kadar bir sıraya konulur. Bu sıraya konulmuş veriler, yani , bir sınama istatistiğinin \n",
    "hesaplanması için kullanılır. Parametresi veya parametreleri verilmiş olasılık dağılımı için birikimli dağılım \n",
    "fonksiyonu kullanılarak bir sıra değerleri bulununur. Bu iki seri kullanılarak önce şu S toplamı elde edilir. Bu toplam \n",
    "kullanılarak Anderson-Darling istatistiği değeri yani elde edilir.Sıfır hipotezde belirtilen olasılık dağılımına göre, \n",
    "elde edilen değerinin belirli bir sabitle (çok kere örneklem hacmi 'N'e bağlı olarak) çarpılması gerektir ve bu değiştirilmiş \n",
    "Anderson-Darling istatistiği adı altında sınama istatistiği olarak kullanılır. sınama istatistiği belirlenen teorik \n",
    "olasılık dağılımı için p-değeri bulmak için kullanılır. Hesaplanmış p-değeri eğer %1 veya %5 olan anlamlılık seviyesinden \n",
    "büyük ise sıfır hipotez kabul edilir ve örneklem verisi belirlenen olasılık dağılımına uyduğu sonucuna varılır. Ancak bu \n",
    "p-değeri bulma işlemi bir olasılık dağılımı simülasyonu gerekeceği için bilgisayarla sayısal hesaplama gerektirir.Bazı \n",
    "olasılık dağılımları için özel tablolar geliştirilmiş ve değişik parametre değerleri ve belirtilmiş anlamlılık değerleri \n",
    "için (genellikle %1 ve %5) kritik değerler tabloda belirtilmiştir. Normal dağılım, log-normal dağılım, üstel dağılım, \n",
    "Weibull dağılımı, logistik dağılım ve Tip I uçsal değerler için bu tabloların bulunduğu bilinmektedir. Tablodan \n",
    "bulunan kritik değer, hesaplanmış değeri ile karşılaştırılır. Belirlenmiş olasılık dağılımına uygunluk sıfır hipotezinin \n",
    "kabul edilmesi sonucudur yani hesaplanmış değer tablo kritik değerinden büyükse örneklem verileri belirlenmiş olasılık \n",
    "dağılımına uygunluk gösterir sonucuna varılır.\"\"\"\n",
    "\n",
    "final_summary = summarize_text_in_chunks(article)\n",
    "processed_summary = post_process_summary(final_summary)\n",
    "print(\"Makale Özeti:\", processed_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b58367fd-d8a8-42f4-9fa0-387026f13518",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n",
      "Your max_length is set to 90, but your input_length is only 67. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=33)\n",
      "Your max_length is set to 90, but your input_length is only 87. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=43)\n",
      "Your max_length is set to 90, but your input_length is only 86. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=43)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Makale Özeti: Ankara, ankara'nın başkenti ve ankara ilinin merkezi olan şehirdir i̇ç anadolu bölgesi'nde yer alır alır. ankara ve civarındaki gerek kamu sektörü özel sektör yatırımları, başka illerden büyük bir nüfus göçünü teşvik etmiştir. cumhuriyetin kuruluşundan günümüze, nüfusu ülke nüfusunun iki katı hızda artmıştır ve artmıştır. türkiye'nin en çok savunma, metal ve motor sektörlerinde yatırım yapılmaktadır. türkiye'nin en çok sayıda üniversiteye sahip ikinci ili olan ve ankara'da ayrıca, üniversite diplomalı kişi oranı ülke ortalamasının iki katıdır. ankara'dan otoyollar, demiryolu ve havayoluyla türkiye'nin diğer şehirlerine ulaşılır tarih öncesi en az 10 bin yıl öncesine, eski taş çağı'na ulaşan öncesine göre tarihi ulaşmıştır. tektosagların ve türkiye cumhuriyeti'nin başkenti gordion, il sınırları içinde yer alır türk kurtuluş savaşı'nın dönüm noktası olan sakarya muharebesi ise polatlı yakınlarında yapılmıştır.\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "import re\n",
    "import warnings \n",
    "warnings.simplefilter(action='ignore', category=Warning)\n",
    "\n",
    "# Eğitilen modeli yükleme\n",
    "summarizer = pipeline(\"summarization\", model=\"./trained_model\", tokenizer=\"./trained_model\")\n",
    "\n",
    "def summarize_text_in_chunks(text, chunk_size=3, max_length=90, min_length=30):\n",
    "    \"\"\"\n",
    "    Metni belirli bir sayıda cümleye bölerek özetler.\n",
    "    \n",
    "    Args:\n",
    "        text (str): Özetlenecek metin.\n",
    "        chunk_size (int): Her bir parçada kaç cümle olacağı.\n",
    "        max_length (int): Özetin maksimum uzunluğu (token sayısı).\n",
    "        min_length (int): Özetin minimum uzunluğu (token sayısı).\n",
    "        \n",
    "    Returns:\n",
    "        str: Parçalanmış ve özetlenmiş metinlerin birleşimi.\n",
    "    \"\"\"\n",
    "    # Metni cümlelere böl\n",
    "    sentences = re.split(r'(?<!\\w\\.\\w.)(?<![A-ZİĞÜŞÖÇ][a-zığüşöç]\\.)(?<=\\.|\\?)\\s', text)\n",
    "    \n",
    "    # Cümleleri gruplara ayır\n",
    "    chunks = [' '.join(sentences[i:i+chunk_size]) for i in range(0, len(sentences), chunk_size)]\n",
    "    \n",
    "    # Her bir parçayı özetle\n",
    "    summaries = []\n",
    "    for chunk in chunks:\n",
    "        summary = summarizer(chunk, max_length=max_length, min_length=min_length, do_sample=False)[0][\"summary_text\"]\n",
    "        summaries.append(summary)\n",
    "    \n",
    "    # Tekrar eden kelimeleri temizle\n",
    "    combined_summary = \" \".join(summaries)\n",
    "    combined_summary = clean_redundancies(combined_summary)\n",
    "    \n",
    "    return combined_summary\n",
    "\n",
    "def clean_redundancies(text):\n",
    "    \"\"\"\n",
    "    Metindeki tekrar eden ifadeleri temizler ve anlamsız tekrarları kaldırır.\n",
    "    \n",
    "    Args:\n",
    "        text (str): Temizlenecek metin.\n",
    "        \n",
    "    Returns:\n",
    "        str: Temizlenmiş metin.\n",
    "    \"\"\"\n",
    "    sentences = re.split(r'(?<!\\w\\.\\w.)(?<![A-ZİĞÜŞÖÇ][a-zığüşöç]\\.)(?<=\\.|\\?)\\s', text)\n",
    "    seen_sentences = set()\n",
    "    cleaned_sentences = []\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        cleaned_sentence = remove_repeated_phrases(sentence)\n",
    "        if cleaned_sentence.lower() not in seen_sentences:\n",
    "            seen_sentences.add(cleaned_sentence.lower())\n",
    "            cleaned_sentences.append(cleaned_sentence)\n",
    "    \n",
    "    return \" \".join(cleaned_sentences)\n",
    "\n",
    "def remove_repeated_phrases(sentence):\n",
    "    \"\"\"\n",
    "    Bir cümledeki tekrar eden kelime öbeklerini temizler.\n",
    "    \n",
    "    Args:\n",
    "        sentence (str): Temizlenecek cümle.\n",
    "        \n",
    "    Returns:\n",
    "        str: Tekrarlar temizlenmiş cümle.\n",
    "    \"\"\"\n",
    "    words = sentence.split()\n",
    "    seen_words = set()\n",
    "    cleaned_words = []\n",
    "    \n",
    "    for word in words:\n",
    "        if word not in seen_words:\n",
    "            seen_words.add(word)\n",
    "            cleaned_words.append(word)\n",
    "    \n",
    "    return \" \".join(cleaned_words)\n",
    "\n",
    "def post_process_summary(summary):\n",
    "    \"\"\"\n",
    "    Özet sonrası işleme fonksiyonu.\n",
    "    \n",
    "    Args:\n",
    "        summary (str): İşlenecek özet.\n",
    "        \n",
    "    Returns:\n",
    "        str: İşlenmiş özet.\n",
    "    \"\"\"\n",
    "    summary = summary.strip()\n",
    "    summary = summary.capitalize()\n",
    "    summary = re.sub(r\" +\", \" \", summary) \n",
    "    summary = re.sub(r\"(?<=[.,])(?=[^\\s])\", \" \", summary)  \n",
    "    return summary\n",
    "\n",
    "article = \"\"\"Ankara (/ˈaŋkaɾa/), Türkiye'nin başkenti ve Ankara ilinin merkezi olan şehirdir.[3] \n",
    "Coğrafi olarak Türkiye'nin merkezine yakın bir konumda bulunur ve İç Anadolu Bölgesi'nde yer alır.\n",
    "Ankara'nın 13 Ekim 1923'te[4] başkent ilan edilmesinin ardından şehir hızla gelişmiş ve Türkiye'nin \n",
    "ikinci en kalabalık ili olmuştur. Türkiye Cumhuriyeti'nin ilk yıllarında ekonomisi tarım ve \n",
    "hayvancılığa dayanan ilin topraklarının yarısı hâlâ tarım amaçlı kullanılmaktadır. Ekonomik etkinlik \n",
    "büyük oranda ticaret ve sanayiye dayalıdır. Tarım ve hayvancılığın ağırlığı ise giderek azalmaktadır. \n",
    "Ankara ve civarındaki gerek kamu sektörü gerek özel sektör yatırımları, başka illerden büyük bir nüfus \n",
    "göçünü teşvik etmiştir. Cumhuriyetin kuruluşundan günümüze, nüfusu ülke nüfusunun iki katı hızda artmıştır. \n",
    "Nüfusun yaklaşık dörtte üçü hizmet sektörü olarak tanımlanabilecek memuriyet, ulaşım, haberleşme ve \n",
    "ticaret benzeri işlerde, dörtte biri sanayide, %2'si ise tarım alanında çalışır. Sanayi, özellikle \n",
    "tekstil, gıda ve inşaat sektörlerinde yoğunlaşmıştır. Günümüzde ise en çok savunma, metal ve motor \n",
    "sektörlerinde yatırım yapılmaktadır. Türkiye'nin en çok sayıda üniversiteye sahip ikinci ili olan[5]\n",
    "Ankara'da ayrıca, üniversite diplomalı kişi oranı ülke ortalamasının iki katıdır. Bu eğitimli nüfus, \n",
    "teknoloji ağırlıklı yatırımların gereksinim duyduğu iş gücünü oluşturur. Ankara'dan otoyollar, \n",
    "demiryolu ve havayoluyla Türkiye'nin diğer şehirlerine ulaşılır.\n",
    "Bilinen tarihi en az 10 bin yıl öncesine, Eski Taş Çağı'na ulaşan[6] Ankara, tarih öncesinden \n",
    "günümüze dek pek çok medeniyeti barındırmıştır. Hititler, Frigyalılar, Lidyalılar, Persler, \n",
    "Galatlar, Romalılar, Bizanslılar, Selçuklular, Osmanlılar ve nihayet Türkiye Cumhuriyeti, il \n",
    "topraklarını kontrolleri altında tutmuştur. Tektosagların ve Türkiye Cumhuriyeti'nin başkenti \n",
    "olan Ankara şehri ve Frigyalıların başkenti Gordion, il sınırları içinde yer alır. Yıldırım \n",
    "Bayezid'in Timur'a yenik düştüğü Ankara Muharebesi Çubuk yakınlarında ve Türk Kurtuluş Savaşı'nın \n",
    "dönüm noktası olan Sakarya Muharebesi ise Polatlı yakınlarında yapılmıştır.\"\"\"\n",
    "\n",
    "final_summary = summarize_text_in_chunks(article)\n",
    "processed_summary = post_process_summary(final_summary)\n",
    "print(\"Makale Özeti:\", processed_summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d77522c8-6db0-4eb5-81da-6b75b0e7f81f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
